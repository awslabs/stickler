{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Structured Object Evaluation\n",
    "\n",
    "This notebook demonstrates how to use the stickler library to evaluate structured data extraction accuracy.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to define structured data models\n",
    "- How to compare individual objects\n",
    "- How to evaluate sets of objects (list comparison)\n",
    "- How to interpret evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stickler.structured_object_evaluator.models.structured_model import StructuredModel\n",
    "from stickler.structured_object_evaluator.models.comparable_field import ComparableField\n",
    "from stickler.comparators.levenshtein import LevenshteinComparator\n",
    "from stickler.structured_object_evaluator.evaluator import StructuredModelEvaluator\n",
    "from typing import List\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Your Data Structure\n",
    "\n",
    "First, let's define a simple invoice structure with comparison rules for each field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Invoice model defined with comparison rules\n"
     ]
    }
   ],
   "source": [
    "class Invoice(StructuredModel):\n",
    "    \"\"\"Simple invoice model for demonstration.\"\"\"\n",
    "\n",
    "    invoice_number: str = ComparableField(\n",
    "        comparator=LevenshteinComparator(),\n",
    "        threshold=0.9,  # Strict matching for invoice numbers\n",
    "        weight=2.0,  # High importance\n",
    "    )\n",
    "\n",
    "    vendor: str = ComparableField(\n",
    "        comparator=LevenshteinComparator(),\n",
    "        threshold=0.7,  # Allow some variation in vendor names\n",
    "        weight=1.0,\n",
    "    )\n",
    "\n",
    "    total: float = ComparableField(\n",
    "        threshold=0.95,  # Very strict for monetary amounts\n",
    "        weight=2.0,  # High importance\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"‚úÖ Invoice model defined with comparison rules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Individual Objects\n",
    "\n",
    "Let's create two invoice objects and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: extra_fields={} invoice_number='INV-2024-001' vendor='ABC Corporation' total=1500.0\n",
      "Prediction:   extra_fields={} invoice_number='INV-2024-001' vendor='ABC Corporation Inc' total=1499.95\n"
     ]
    }
   ],
   "source": [
    "# Create ground truth and prediction\n",
    "ground_truth = Invoice(\n",
    "    invoice_number=\"INV-2024-001\", vendor=\"ABC Corporation\", total=1500.00\n",
    ")\n",
    "\n",
    "prediction = Invoice(\n",
    "    invoice_number=\"INV-2024-001\",  # Perfect match\n",
    "    vendor=\"ABC Corporation Inc\",  # Close match - should pass 0.7 threshold\n",
    "    total=1499.95,  # Close match - should pass 0.95 threshold\n",
    ")\n",
    "\n",
    "print(\"Ground Truth:\", ground_truth)\n",
    "print(\"Prediction:  \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Comparison Results:\n",
      "Overall Score: 0.558\n",
      "All Fields Matched: False\n",
      "\n",
      "üìã Field-by-Field Scores:\n",
      "  invoice_number : 1.000\n",
      "  vendor         : 0.789\n",
      "  total          : 0.000\n"
     ]
    }
   ],
   "source": [
    "# Compare using the built-in compare_with method\n",
    "result = ground_truth.compare_with(prediction, include_confusion_matrix=True)\n",
    "\n",
    "print(f\"\\nüìä Comparison Results:\")\n",
    "print(f\"Overall Score: {result['overall_score']:.3f}\")\n",
    "print(f\"All Fields Matched: {result['all_fields_matched']}\")\n",
    "\n",
    "print(f\"\\nüìã Field-by-Field Scores:\")\n",
    "for field, score in result[\"field_scores\"].items():\n",
    "    print(f\"  {field:15}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the Evaluator for Detailed Analysis\n",
    "\n",
    "The evaluator provides additional metrics like precision, recall, and F1 scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Detailed Evaluation Metrics:\n",
      "Overall Precision: 0.667\n",
      "Overall Recall:    1.000\n",
      "Overall F1 Score:  0.800\n",
      "Overall ANLS:      0.558\n",
      "\n",
      "üìã Field-Level Analysis:\n",
      "  invoice_number : ANLS 1.000\n",
      "  vendor         : ANLS 0.789\n",
      "  total          : ANLS 0.000\n"
     ]
    }
   ],
   "source": [
    "# Use the evaluator for detailed analysis\n",
    "evaluator = StructuredModelEvaluator()\n",
    "eval_result = evaluator.evaluate(ground_truth, prediction)\n",
    "\n",
    "print(\"üìà Detailed Evaluation Metrics:\")\n",
    "print(f\"Overall Precision: {eval_result['overall']['precision']:.3f}\")\n",
    "print(f\"Overall Recall:    {eval_result['overall']['recall']:.3f}\")\n",
    "print(f\"Overall F1 Score:  {eval_result['overall']['f1']:.3f}\")\n",
    "print(f\"Overall ANLS:      {eval_result['overall']['anls_score']:.3f}\")\n",
    "\n",
    "print(f\"\\nüìã Field-Level Analysis:\")\n",
    "for field, metrics in eval_result[\"fields\"].items():\n",
    "    if isinstance(metrics, dict) and \"anls_score\" in metrics:\n",
    "        print(f\"  {field:15}: ANLS {metrics['anls_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. List Comparison - The Real Power\n",
    "\n",
    "Now let's see how to compare lists of objects, which is where this library really shines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ InvoiceBatch model defined\n"
     ]
    }
   ],
   "source": [
    "# Define a model that contains lists of invoices\n",
    "class InvoiceBatch(StructuredModel):\n",
    "    \"\"\"A batch of invoices for processing.\"\"\"\n",
    "\n",
    "    batch_id: str = ComparableField(\n",
    "        comparator=LevenshteinComparator(), threshold=0.9, weight=1.0\n",
    "    )\n",
    "\n",
    "    invoices: List[Invoice] = ComparableField(\n",
    "        weight=3.0  # This is the most important field\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"‚úÖ InvoiceBatch model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Batch:\n",
      "  1. INV-001 | Company A | $1000.0\n",
      "  2. INV-002 | Company B | $2000.0\n",
      "  3. INV-003 | Company C | $1500.0\n",
      "\n",
      "Predicted Batch:\n",
      "  1. INV-001 | Company A | $1000.0\n",
      "  2. INV-002 | Company B Ltd | $2000.0\n",
      "  3. INV-004 | Company D | $1800.0\n"
     ]
    }
   ],
   "source": [
    "# Create ground truth batch\n",
    "gt_batch = InvoiceBatch(\n",
    "    batch_id=\"BATCH-2024-001\",\n",
    "    invoices=[\n",
    "        Invoice(invoice_number=\"INV-001\", vendor=\"Company A\", total=1000.00),\n",
    "        Invoice(invoice_number=\"INV-002\", vendor=\"Company B\", total=2000.00),\n",
    "        Invoice(invoice_number=\"INV-003\", vendor=\"Company C\", total=1500.00),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create prediction batch with some differences\n",
    "pred_batch = InvoiceBatch(\n",
    "    batch_id=\"BATCH-2024-001\",\n",
    "    invoices=[\n",
    "        Invoice(\n",
    "            invoice_number=\"INV-001\", vendor=\"Company A\", total=1000.00\n",
    "        ),  # Perfect match\n",
    "        Invoice(\n",
    "            invoice_number=\"INV-002\", vendor=\"Company B Ltd\", total=2000.00\n",
    "        ),  # Slight vendor difference\n",
    "        Invoice(\n",
    "            invoice_number=\"INV-004\", vendor=\"Company D\", total=1800.00\n",
    "        ),  # Different invoice entirely\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Ground Truth Batch:\")\n",
    "for i, inv in enumerate(gt_batch.invoices):\n",
    "    print(f\"  {i + 1}. {inv.invoice_number} | {inv.vendor} | ${inv.total}\")\n",
    "\n",
    "print(\"\\nPredicted Batch:\")\n",
    "for i, inv in enumerate(pred_batch.invoices):\n",
    "    print(f\"  {i + 1}. {inv.invoice_number} | {inv.vendor} | ${inv.total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Batch Comparison Results:\n",
      "Overall Score: 0.948\n",
      "All Fields Matched: True\n",
      "\n",
      "üìä Field Scores:\n",
      "  batch_id       : 1.000\n",
      "  invoices       : 0.931\n"
     ]
    }
   ],
   "source": [
    "# Compare the batches\n",
    "batch_result = gt_batch.compare_with(pred_batch, include_confusion_matrix=True)\n",
    "\n",
    "print(\"üîç Batch Comparison Results:\")\n",
    "print(f\"Overall Score: {batch_result['overall_score']:.3f}\")\n",
    "print(f\"All Fields Matched: {batch_result['all_fields_matched']}\")\n",
    "\n",
    "print(f\"\\nüìä Field Scores:\")\n",
    "for field, score in batch_result[\"field_scores\"].items():\n",
    "    print(f\"  {field:15}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Detailed Batch Evaluation:\n",
      "Overall Precision: 1.000\n",
      "Overall Recall:    1.000\n",
      "Overall F1 Score:  1.000\n",
      "\n",
      "üìã Field-Level Analysis:\n",
      "  batch_id       : ANLS 1.000\n",
      "  invoices       : ANLS 0.931 (list field)\n"
     ]
    }
   ],
   "source": [
    "# Use evaluator for detailed batch analysis\n",
    "batch_eval_result = evaluator.evaluate(gt_batch, pred_batch)\n",
    "\n",
    "print(\"üìà Detailed Batch Evaluation:\")\n",
    "print(f\"Overall Precision: {batch_eval_result['overall']['precision']:.3f}\")\n",
    "print(f\"Overall Recall:    {batch_eval_result['overall']['recall']:.3f}\")\n",
    "print(f\"Overall F1 Score:  {batch_eval_result['overall']['f1']:.3f}\")\n",
    "\n",
    "print(f\"\\nüìã Field-Level Analysis:\")\n",
    "for field, metrics in batch_eval_result[\"fields\"].items():\n",
    "    if isinstance(metrics, dict):\n",
    "        if \"anls_score\" in metrics:\n",
    "            print(f\"  {field:15}: ANLS {metrics['anls_score']:.3f}\")\n",
    "        elif \"overall\" in metrics and \"anls_score\" in metrics[\"overall\"]:\n",
    "            print(\n",
    "                f\"  {field:15}: ANLS {metrics['overall']['anls_score']:.3f} (list field)\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding List Comparison\n",
    "\n",
    "The library uses Hungarian algorithm to optimally match objects in lists. Let's see what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ List Comparison Analysis:\n",
      "True Positives (TP):  0 (correctly matched invoices)\n",
      "False Positives (FP): 0 (incorrect matches + extra predictions)\n",
      "False Negatives (FN): 0 (missed ground truth invoices)\n",
      "\n",
      "üìà Derived Metrics:\n",
      "Precision: 0.000\n",
      "Recall:    0.000\n",
      "F1 Score:  0.000\n"
     ]
    }
   ],
   "source": [
    "# Get confusion matrix for detailed analysis\n",
    "cm = batch_result.get(\"confusion_matrix\", {})\n",
    "if cm and \"fields\" in cm and \"invoices\" in cm[\"fields\"]:\n",
    "    invoice_metrics = cm[\"fields\"][\"invoices\"]\n",
    "\n",
    "    print(\"üéØ List Comparison Analysis:\")\n",
    "    print(\n",
    "        f\"True Positives (TP):  {invoice_metrics.get('tp', 0)} (correctly matched invoices)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"False Positives (FP): {invoice_metrics.get('fp', 0)} (incorrect matches + extra predictions)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"False Negatives (FN): {invoice_metrics.get('fn', 0)} (missed ground truth invoices)\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüìà Derived Metrics:\")\n",
    "    derived = invoice_metrics.get(\"derived\", {})\n",
    "    print(f\"Precision: {derived.get('cm_precision', 0):.3f}\")\n",
    "    print(f\"Recall:    {derived.get('cm_recall', 0):.3f}\")\n",
    "    print(f\"F1 Score:  {derived.get('cm_f1', 0):.3f}\")\n",
    "else:\n",
    "    print(\"Confusion matrix data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Beautiful Results Display\n",
    "\n",
    "The library includes beautiful pretty printing functions for displaying results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Beautiful Results Display:\n",
      "==================================================\n",
      "=== CONFUSION MATRIX SUMMARY ===\n",
      "\n",
      "--- Raw Counts ---\n",
      "Metric             Count\n",
      "-------------------------\n",
      "True Positive          4\n",
      "False Positive         0\n",
      "True Negative          0\n",
      "False Negative         0\n",
      "False Discovery        0\n",
      "\n",
      "--- Derived Metrics ---\n",
      "Metric               Value Visual                \n",
      "--------------------------------------------------\n",
      "Precision          100.00% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  \n",
      "Recall             100.00% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  \n",
      "F1 Score           100.00% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  \n",
      "Accuracy           100.00% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  \n",
      "\n",
      "\n",
      "=== FIELD-LEVEL METRICS ===\n",
      "\n",
      "Field                    TP     FP     TN     FN     FD     Prec   Recall       F1      Acc Visual                \n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "batch_id                  1      0      0      0      0  100.00%  100.00%  100.00%  100.00% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  \n",
      "\n",
      "invoices                  3      0      0      0      0  100.00%  100.00%  100.00%  100.00% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  \n",
      "\n",
      "\n",
      "\n",
      "=== CONFUSION MATRIX VISUALIZATION ===\n",
      "\n",
      "TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n",
      "\n",
      "Legend:\n",
      "  T True Positive (TP): 4 (100.00%)\n",
      "  N True Negative (TN): 0 (0.00%)\n",
      "  F False Positive (FP): 0 (0.00%)\n",
      "  M False Negative (FN): 0 (0.00%)\n",
      "  D False Discovery (FD): 0 (0.00%)\n",
      "\n",
      "\n",
      "\n",
      "üéØ The pretty printer works with ANY evaluation result:\n",
      "‚Ä¢ Individual comparisons: model.compare_with()\n",
      "‚Ä¢ Evaluator results: evaluator.evaluate()\n",
      "‚Ä¢ Bulk evaluator results: bulk_evaluator.compute()\n",
      "‚Ä¢ Includes colors, visual bars, and detailed breakdowns!\n"
     ]
    }
   ],
   "source": [
    "# Import the beautiful print function\n",
    "from stickler.structured_object_evaluator.utils.pretty_print import (\n",
    "    print_confusion_matrix,\n",
    ")\n",
    "\n",
    "print(\"üé® Beautiful Results Display:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use the pretty printer with our batch comparison results\n",
    "print_confusion_matrix(batch_eval_result, show_details=True)\n",
    "\n",
    "print(\"\\nüéØ The pretty printer works with ANY evaluation result:\")\n",
    "print(\"‚Ä¢ Individual comparisons: model.compare_with()\")\n",
    "print(\"‚Ä¢ Evaluator results: evaluator.evaluate()\")\n",
    "print(\"‚Ä¢ Bulk evaluator results: bulk_evaluator.compute()\")\n",
    "print(\"‚Ä¢ Includes colors, visual bars, and detailed breakdowns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "üéØ **What We Learned:**\n",
    "\n",
    "1. **Individual Object Comparison**: Simple field-by-field scoring\n",
    "2. **List Comparison**: Optimal matching using Hungarian algorithm \n",
    "3. **Flexible Thresholds**: Different comparison rules per field type\n",
    "4. **Weighted Importance**: Critical fields can have higher impact\n",
    "5. **Multiple Metrics**: ANLS scores, precision/recall, confusion matrices\n",
    "6. **Beautiful Output**: `print_confusion_matrix()` for gorgeous results display\n",
    "\n",
    "üöÄ **Perfect for:**\n",
    "- Document extraction evaluation (invoices, receipts, forms)\n",
    "- Entity extraction assessment\n",
    "- OCR quality measurement\n",
    "- ML model evaluation on structured outputs\n",
    "\n",
    "üìö **Next Steps:**\n",
    "1. Try with your own data structures\n",
    "2. Experiment with different comparators and thresholds\n",
    "3. Use the non-match analysis for debugging\n",
    "4. Scale up to larger datasets with the bulk evaluator\n",
    "5. Use `print_confusion_matrix()` for beautiful results display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-idp",
   "language": "python",
   "name": "agentic-idp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
